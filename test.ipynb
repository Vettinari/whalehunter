{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add processing that will analyse the _mapped data and will output boolean column for \"filled incorrectly\"\n",
    "raw_df = pd.read_parquet(\"data/raw/corrected_raw_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from exploration.utils import (\n",
    "    cast_to_string,\n",
    "    gender_map,\n",
    "    marital_status_map,\n",
    "    optimize_dtypes,\n",
    "    scale_with_max_value,\n",
    "    day_name_to_num,\n",
    "    cast_ints_and_floats,\n",
    "    exclude_user_hourly_cum_perc_revenue_columns,\n",
    "    scale_and_log_numerical_df,\n",
    "    encode_categorical_features,\n",
    "    convert_to_absolute_values,\n",
    ")\n",
    "from exploration.column_config import (\n",
    "    cum_perc_revenue_map,\n",
    "    drop_columns,\n",
    "    max_scalers,\n",
    "    one_hot_columns,\n",
    "    high_cardinality_columns,\n",
    "    log_scalers,\n",
    "    targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milos\\Projects\\WhaleHunter\\venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA rows number: 10\n"
     ]
    }
   ],
   "source": [
    "df = raw_df.copy(True)\n",
    "df = df.rename(columns=cum_perc_revenue_map)\n",
    "cols = sorted([col for col in df.columns if col not in drop_columns])\n",
    "df = df[cols]\n",
    "df = exclude_user_hourly_cum_perc_revenue_columns(df)\n",
    "\n",
    "df[\"payout\"] = df[\"payout\"].astype(float).round(2)\n",
    "df = cast_to_string(df)\n",
    "df[\"user_gender\"] = df[\"user_gender\"].apply(gender_map)\n",
    "df[\"user_marital_status\"] = df[\"user_marital_status\"].apply(marital_status_map)\n",
    "df[\"user_age\"] = df[\"user_age\"].clip(lower=16, upper=100)\n",
    "df[\"user_registration_day_of_week\"] = df[\"user_registration_day_of_week\"].apply(\n",
    "    day_name_to_num\n",
    ")\n",
    "df[\"user_registration_year\"] = (\n",
    "    df[\"user_registration_year\"] - df[\"user_registration_year\"].min()\n",
    ")\n",
    "\n",
    "# Creating aboslute values\n",
    "df = convert_to_absolute_values(df)\n",
    "df = df.fillna(0)\n",
    "\n",
    "for scaler in max_scalers:\n",
    "    df = scale_with_max_value(df, scaler)\n",
    "\n",
    "df.to_parquet(\"data/raw/_pre_scaled_data.parquet\")\n",
    "\n",
    "# numerical_df\n",
    "log_df = df[log_scalers].copy(True)\n",
    "\n",
    "# proces assymetricla log columns\n",
    "col = \"diff_avg_user_order_revenue_usd_24h_48h_vs_0h_24h\"\n",
    "log_df[f\"positive_{col}\"] = np.where(log_df[col].values >= 0, log_df[col].values, 0)\n",
    "log_df[f\"negative_{col}\"] = np.where(\n",
    "    log_df[col].values < 0, np.abs(log_df[col].values), 0\n",
    ")\n",
    "log_df = log_df.drop(columns=[col])\n",
    "\n",
    "# user_registration_confirmation_minutes_diff\n",
    "log_df[\"user_registration_confirmation_minutes_diff\"] = log_df[\n",
    "    \"user_registration_confirmation_minutes_diff\"\n",
    "].clip(\n",
    "    lower=0, upper=log_df[\"user_registration_confirmation_minutes_diff\"].quantile(0.99)\n",
    ")\n",
    "\n",
    "log_df = cast_ints_and_floats(log_df)\n",
    "log_df = scale_and_log_numerical_df(log_df)\n",
    "\n",
    "# categorical_df\n",
    "one_hot_df = df[one_hot_columns].copy()\n",
    "one_hot_df = pd.get_dummies(\n",
    "    one_hot_df,\n",
    "    columns=one_hot_columns,\n",
    "    prefix=one_hot_columns,\n",
    "    prefix_sep=\"_\",\n",
    "    dummy_na=False,\n",
    "    drop_first=True,\n",
    ")\n",
    "\n",
    "# high cardinality columns\n",
    "high_df = encode_categorical_features(\n",
    "    df=df[high_cardinality_columns],\n",
    "    encoding_type=\"frequency\",\n",
    ")\n",
    "\n",
    "df = pd.concat([log_df, one_hot_df, high_df], axis=1)\n",
    "df = df[sorted(df.columns)]\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_parquet(\"data/scaled_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_users_30d = df[\"user_revenue_usd_30d\"] > 0\n",
    "filter_users_60d = df[\"user_revenue_usd_60d\"] > 0\n",
    "filter_users_90d = df[\"user_revenue_usd_90d\"] > 0\n",
    "\n",
    "df_users_30d = df[filter_users_30d].reset_index(drop=True)\n",
    "df_users_60d = df[filter_users_60d].reset_index(drop=True)\n",
    "df_users_90d = df[filter_users_90d].reset_index(drop=True)\n",
    "\n",
    "non_converted_users_30d = df[~filter_users_30d]\n",
    "non_converted_users_60d = df[~filter_users_60d]\n",
    "non_converted_users_90d = df[~filter_users_90d]\n",
    "\n",
    "sample_non_converted_users_30d = non_converted_users_30d.sample(\n",
    "    df_users_30d.shape[0], random_state=42\n",
    ")\n",
    "sample_non_converted_users_60d = non_converted_users_60d.sample(\n",
    "    df_users_60d.shape[0], random_state=42\n",
    ")\n",
    "sample_non_converted_users_90d = non_converted_users_90d.sample(\n",
    "    df_users_90d.shape[0], random_state=42\n",
    ")\n",
    "\n",
    "df_users_30d_balanced = pd.concat(\n",
    "    [df_users_30d, sample_non_converted_users_30d]\n",
    ").reset_index(drop=True)\n",
    "df_users_60d_balanced = pd.concat(\n",
    "    [df_users_60d, sample_non_converted_users_60d]\n",
    ").reset_index(drop=True)\n",
    "df_users_90d_balanced = pd.concat(\n",
    "    [df_users_90d, sample_non_converted_users_90d]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_users_30d.drop(\n",
    "        columns=[\"user_revenue_usd_60d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_30d.parquet\")\n",
    ")\n",
    "(\n",
    "    df_users_60d.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_60d.parquet\")\n",
    ")\n",
    "(\n",
    "    df_users_90d.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_60d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_90d.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_users_30d_balanced.drop(\n",
    "        columns=[\"user_revenue_usd_60d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_30d_balanced.parquet\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_users_60d_balanced.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_60d_balanced.parquet\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_users_90d_balanced.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_60d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_90d_balanced.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample_size = int(df_users_30d.shape[0] / 10)\n",
    "sample_non_converted_users_30d_small = non_converted_users_30d.sample(\n",
    "    small_sample_size, random_state=42\n",
    ")\n",
    "sample_non_converted_users_60d_small = non_converted_users_60d.sample(\n",
    "    small_sample_size, random_state=42\n",
    ")\n",
    "sample_non_converted_users_90d_small = non_converted_users_90d.sample(\n",
    "    small_sample_size, random_state=42\n",
    ")\n",
    "\n",
    "df_users_30d_balanced_small = pd.concat(\n",
    "    [df_users_30d, sample_non_converted_users_30d_small]\n",
    ").reset_index(drop=True)\n",
    "df_users_60d_balanced_small = pd.concat(\n",
    "    [df_users_60d, sample_non_converted_users_60d_small]\n",
    ").reset_index(drop=True)\n",
    "df_users_90d_balanced_small = pd.concat(\n",
    "    [df_users_90d, sample_non_converted_users_90d_small]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "(\n",
    "    df_users_30d_balanced_small.drop(\n",
    "        columns=[\"user_revenue_usd_60d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_30d_balanced_small.parquet\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_users_60d_balanced_small.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_90d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_60d_balanced_small.parquet\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_users_90d_balanced_small.drop(\n",
    "        columns=[\"user_revenue_usd_30d\", \"user_revenue_usd_60d\"]\n",
    "    ).to_parquet(\"data/scaled_revenue_90d_balanced_small.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
